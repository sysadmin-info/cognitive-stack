# Konfiguracja providerów API
# Klucze ładowane z pliku .env (lub zmiennych środowiskowych)

providers:
  openai:
    enabled: true
    api_key: "${OPENAI_API_KEY}"
    model: "${MODEL_NAME_OPENAI:gpt-4o}"  # fallback jeśli nie ustawiono
    base_url: "${OPENAI_API_URL:https://api.openai.com/v1}"
    max_tokens: 4096
    temperature: 0.7

  anthropic:
    enabled: true
    api_key: "${CLAUDE_API_KEY}"  # lub ANTHROPIC_API_KEY
    model: "${MODEL_NAME_CLAUDE:claude-sonnet-4-20250514}"
    base_url: "https://api.anthropic.com"
    max_tokens: 4096
    temperature: 0.7

  google:
    enabled: true
    api_key: "${GEMINI_API_KEY}"
    model: "${MODEL_NAME_GEMINI:gemini-1.5-pro-latest}"
    base_url: "https://generativelanguage.googleapis.com/v1beta"
    max_tokens: 4096
    temperature: 0.7

  # Opcjonalnie: lokalne modele przez Ollama
  ollama:
    enabled: false
    model: "llama3.1:70b"
    base_url: "http://localhost:11434"
    max_tokens: 4096
    temperature: 0.7

  # Opcjonalnie: LM Studio (OpenAI-compatible local server)
  lmstudio:
    enabled: true
    api_key: "${LMSTUDIO_API_KEY:local}"
    model: "${MODEL_NAME_LM:google/gemma-3-27b}"
    base_url: "${LMSTUDIO_API_URL:http://169.254.83.107:1234/v1}"
    max_tokens: 4096
    temperature: 0.7

  # Opcjonalnie: Anything LLM (OpenAI-compatible)
  anythingllm:
    enabled: true
    api_key: "${ANYTHING_API_KEY:local}"
    model: "${MODEL_NAME_ANY:qwen3-asteria-14b-128k}"
    base_url: "${ANYTHING_API_URL:http://169.254.83.107:1234/v1}"
    max_tokens: 4096
    temperature: 0.7

# Które modele używać domyślnie w radzie
default_council:
  - openai
  - anthropic
  - google
  - lmstudio
  - anythingllm

# Timeout dla zapytań API (sekundy)
timeout: 360

# Retry przy błędach
max_retries: 3
